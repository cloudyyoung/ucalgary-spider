{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import SpanRuler, EntityRuler\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from nltk import Tree\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_generator():\n",
    "    yield from itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "letters = letter_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mongodb\n",
    "client = MongoClient(\"mongodb://root:password@localhost:27017/\")\n",
    "catalog = client.get_database(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by length of title\n",
    "subject_codes_docs = list(catalog.get_collection(\"subject_codes\").find())\n",
    "subject_codes_docs.sort(key=lambda x: len(x[\"title\"]), reverse=True)\n",
    "subject_codes_map = {doc[\"title\"]: doc[\"code\"] for doc in subject_codes_docs}\n",
    "subject_codes = [doc[\"code\"] for doc in subject_codes_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base patterns\n",
    "subject_code_regex = r\"([A-Z]{3,4})\"  # ART, MATH\n",
    "course_number_regex = r\"(\\d{2}-\\d|\\d{3}\\.\\d{1,2}|\\d{2,3})\"  # 101, 30-1, 599.45\n",
    "course_code_regex = rf\"{subject_code_regex} {course_number_regex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject_code(sentence: str, loose: bool=False):\n",
    "\tfor subject_code in subject_codes_docs:\n",
    "\t\tif loose:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]}\", rf\"{subject_code[\"code\"]}\", sentence)\n",
    "\t\telse:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]} {course_number_regex}\", rf\"{subject_code[\"code\"]} \\1\", sentence)\n",
    "\treturn sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_letter():\n",
    "    # Create an iterator that cycles through the alphabet\n",
    "    for letter in itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "        yield letter\n",
    "\n",
    "replacement_letters = get_replacement_letter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension(\"course_code\", default=None, force=True)\n",
    "Doc.set_extension(\"replacements\", default=[], force=True)\n",
    "Doc.set_extension(\"json_logics\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "expand_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "constituency_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 1235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc:\n",
    "        token.is_sent_start = False\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "            \n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"fix_ent_head\")\n",
    "def fix_ent_head(doc: Doc):\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "                ancestors = list(filter(lambda x: x.text in subject_codes, token.ancestors))\n",
    "                ancestor = ancestors[0] if len(ancestors) > 0 else None\n",
    "\n",
    "                if ancestor:\n",
    "                    token.head = ancestor\n",
    "                    token._.course_code = f\"{ancestor.text}{token.text}\"\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"expand_course_code\")\n",
    "def expand_course_code(doc: Doc):\n",
    "    sent = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in subject_codes:\n",
    "            continue\n",
    "\n",
    "        elif token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "            left_tokens = [token.head] + list(reversed(list(doc[: token.i])))\n",
    "\n",
    "            for left_token in left_tokens:\n",
    "                if left_token.text in subject_codes:\n",
    "                    sent += left_token.text_with_ws\n",
    "                    break\n",
    "\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "        else:\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc.ents = []\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "    {\n",
    "        \"label\": \"COURSE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": {\"REGEX\": subject_code_regex}},\n",
    "            {\"TEXT\": {\"REGEX\": course_number_regex}},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"REQUISITE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": \"RQ\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Z]\"}},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "entity_ruler.clear()\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "@Language.component(\"detect_entity\")\n",
    "def detect_entity(doc: Doc):\n",
    "    ents = entity_ruler.match(doc)\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"merge_entity_spans\")\n",
    "def merge_entity_spans(doc: Doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ is not None:\n",
    "                retokenizer.merge(ent, attrs={\"ENT_TYPE\": ent.label_, \"ENT_IOB\": \"B\", \"ENT_IOE\": \"E\", \"ENT_IOR\": \"\", \"pos\": \"PROPN\"})\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'fix_ent_head',\n",
       " 'expand_course_code',\n",
       " 'detect_entity',\n",
       " 'merge_entity_spans']"
      ]
     },
     "execution_count": 1240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_nlp.add_pipe(\"fix_ent_head\")\n",
    "expand_nlp.add_pipe(\"expand_course_code\")\n",
    "expand_nlp.add_pipe(\"detect_entity\")\n",
    "expand_nlp.add_pipe(\"merge_entity_spans\")\n",
    "\n",
    "expand_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeating_array(head: list, repeat_elemnts: list, repeat_times: int, tail: list):\n",
    "    repeated_part = repeat_elemnts * repeat_times\n",
    "    return head + repeated_part + tail\n",
    "\n",
    "def get_dynamic_patterns(head: list, repeat_tokens: list, repeat_range: range, tail: list):\n",
    "    patterns = []\n",
    "    for i in repeat_range:\n",
    "        patterns.append(get_repeating_array(head, repeat_tokens, i, tail))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "### X Units of\n",
    "def x_units_of(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    units_required = int(span[0].text)\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"units\": {\n",
    "            \"required\": units_required,\n",
    "            \"from\": [course.text for course in courses],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "x_units_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"IS_DIGIT\": True},\n",
    "        {\"LEMMA\": \"unit\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \"or\", \",\"]}},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\"X units of\", x_units_of_patterns, greedy=\"LONGEST\", on_match=x_units_of)\n",
    "### X Units of\n",
    "\n",
    "\n",
    "### One of\n",
    "def one_of(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"or\": [course.text for course in courses],\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "one_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"LEMMA\": \"one\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \",\"]}},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\"One of\", one_of_patterns, greedy=\"LONGEST\", on_match=one_of)\n",
    "### One of\n",
    "\n",
    "\n",
    "### Consent of\n",
    "def consent_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    consent_of = span[2:]\n",
    "    json_logic = {\"consent\": consent_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Consent of\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"consent\"},\n",
    "            {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Za-z, ]\"}, \"OP\": \"*\"},\n",
    "            {\"IS_SENT_START\": False},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=consent_of,\n",
    ")\n",
    "\n",
    "def admission_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    admission_of = span[2:]\n",
    "    json_logic = {\"admission\": admission_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Admission to\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"admission\"},\n",
    "            {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Za-z, ]\", \"NOT_IN\": [\"and\", \"or\"]}, \"OP\": \"*\"},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=admission_of,\n",
    ")\n",
    "### Consent of\n",
    "\n",
    "\n",
    "### Both A and B\n",
    "def both_a_and_b(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    a = span[1]\n",
    "    b = span[3]\n",
    "    json_logic = {\"and\": [a.text, b.text]}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Both A and B\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"both\"},\n",
    "            {\"IS_ALPHA\": True},\n",
    "            {\"LEMMA\": \"and\"},\n",
    "            {\"IS_ALPHA\": True},\n",
    "            {\"IS_SENT_START\": False},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=both_a_and_b,\n",
    ")\n",
    "### Both A and B\n",
    "\n",
    "\n",
    "\n",
    "@Language.component(\"constitute_requisite\")\n",
    "def constitute_requisite(doc: Doc):\n",
    "    sent = doc.text\n",
    "    matches = matcher(doc)\n",
    "    replacements = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        letter = next(letters)\n",
    "        replacement = f\"RQ {letter}\"\n",
    "\n",
    "        span = doc[start:end]\n",
    "        replacements.append((replacement, span))\n",
    "\n",
    "        sent = re.sub(re.escape(span.text), replacement, sent)\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc._.replacements = replacements\n",
    "    new_doc._.json_logics = doc._.json_logics\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'constitute_requisite',\n",
       " 'detect_entity',\n",
       " 'merge_entity_spans']"
      ]
     },
     "execution_count": 1243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constituency_nlp.add_pipe(\"constitute_requisite\")\n",
    "constituency_nlp.add_pipe(\"detect_entity\")\n",
    "constituency_nlp.add_pipe(\"merge_entity_spans\")\n",
    "\n",
    "constituency_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_format(tok):\n",
    "    # return \"_\".join([tok.orth_, tok.tag_])\n",
    "    return f\"{tok.orth_} ({tok.dep_})\"\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replacement(key: str, replacements: list[tuple[str, Span]]):\n",
    "    # Remove trailing period\n",
    "    key = key[:-1] if key[-1] == \".\" else key\n",
    "    \n",
    "    for _key, span in replacements:\n",
    "        if key == _key:\n",
    "            return span\n",
    "\n",
    "def find_json_logic(span: Span, json_logic: list[tuple[Span, dict]]):\n",
    "    for _span, logic in json_logic:\n",
    "        if span.text == _span.text:\n",
    "            return logic\n",
    "\n",
    "def extract_sentence(sent: Span, replacements: dict[str, Span], json_logics: list[tuple[Span, dict]]):\n",
    "    logic_operator = \"and\"\n",
    "    conditions = []\n",
    "\n",
    "    for token in sent:\n",
    "        if token.text.lower() in [\"and\", \"or\"]:\n",
    "            logic_operator = token.text.lower()\n",
    "\n",
    "        elif token.ent_type_ == \"COURSE\":\n",
    "            conditions.append({\"course\": token.text})\n",
    "\n",
    "        elif token.ent_type_ == \"REQUISITE\":\n",
    "            replacement = find_replacement(token.text, replacements)\n",
    "            json_logic = find_json_logic(replacement, json_logics)\n",
    "            conditions.append(json_logic)\n",
    "\n",
    "    if len(conditions) == 1:\n",
    "        return conditions[0]\n",
    "\n",
    "    return {\n",
    "        logic_operator: conditions\n",
    "    }\n",
    "\n",
    "def extract_doc(doc: Doc):\n",
    "    logic_operator = \"and\"\n",
    "    conditions = []\n",
    "\n",
    "    # In the last sentence, find coordinating conjunctions\n",
    "    last_sentence = list(doc.sents)[-1]\n",
    "    last_sentence_first_word = last_sentence[0]\n",
    "\n",
    "    if last_sentence_first_word.dep_ == \"cc\" or last_sentence_first_word.pos_ == \"CCONJ\":\n",
    "        logic_operator = last_sentence_first_word.text.lower()\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        conditions.append(extract_sentence(sent, doc._.replacements, doc._.json_logics))\n",
    "\n",
    "    if len(conditions) == 1:\n",
    "        return conditions[0]\n",
    "    \n",
    "    return {\n",
    "        logic_operator: conditions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: MATH 209 and admission to the Energy Engineering program.\n",
      "Expand  : MATH 209 and admission to the Energy Engineering program.\n",
      "Constituency: MATH 209 and RQ D.\n",
      "(MATH 209, RQ D.)\n",
      "[('RQ D', admission to the Energy Engineering program)]\n",
      "[(admission to the Energy Engineering program, {'admission': the Energy Engineering program})]\n",
      "RQ D admission to the Energy Engineering program []\n",
      "{'and': [{'course': 'MATH 209'}, {'admission': the Energy Engineering program}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MATH 209\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COURSE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RQ D.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">REQUISITE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"dd493bf0f9f94243852099573c9b0848-0\" class=\"displacy\" width=\"350\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">MATH 209</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">RQ D.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd493bf0f9f94243852099573c9b0848-0-0\" stroke-width=\"2px\" d=\"M62,102.0 62,85.33333333333333 147.0,85.33333333333333 147.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd493bf0f9f94243852099573c9b0848-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M147.0,104.0 L151.0,96.0 143.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd493bf0f9f94243852099573c9b0848-0-1\" stroke-width=\"2px\" d=\"M62,102.0 62,68.66666666666666 250.0,68.66666666666666 250.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd493bf0f9f94243852099573c9b0848-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250.0,104.0 L254.0,96.0 246.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sent = \"Actuarial Science 327; Statistics 323; 3 units from Mathematics 311, 313, 367 or 375; and 3 units from Computer Science 217, 231, 235 or Data Science 211.\"\n",
    "# sent = \"CPSC 457 and 3 units from SENG 300, 301 or ENSF 480; and admission to the Schulich School of Engineering.\"\n",
    "# sent = \"SGMA 395 or ENTI 317 or 381.\"\n",
    "# sent = \"One of FILM 321 or 323 and one of FILM 331 or 333.\"\n",
    "# sent = \"FILM 331 or 333.\"\n",
    "# sent = \"ENCI 473; and ENGG 319 or ENDG 319.\"\n",
    "# sent = \"3 units from ENCI 481, ENEE 377 or 519.09.\"\n",
    "# sent = \"ENEL 341, BMEN 327 or ENGG 225.\"\n",
    "# sent = \"ENEL 471; and one of BMEN 319 or ENGG 319 or ENEL 419.\"\n",
    "# sent = \"3 units from ENGG 319, ENDG 319 or ENEL 419.\"\n",
    "# sent = \"FILM 201 and 3 units from 305 or 321.\"\n",
    "# sent = \"INDG 201 and 3 units from INDG 303 or 345.\"\n",
    "# sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"STAT 205 or 213; and admission to the Kinesiology Honours program; and consent of the Faculty.\"\n",
    "sent = \"MATH 209 and admission to the Energy Engineering program.\"\n",
    "# sent = \"Both MATH 349 and 353; or both MATH 283 and 381; or MATH 267.\"\n",
    "# sent = \"MATH 431 or PMAT 431; MATH 429 or PMAT 429 or MATH 327 or PMAT 427.\"\n",
    "# sent = \"MATH 445 or 447; 3 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MATH 383; and 6 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MRSC 451 and consent of the Department.\"\n",
    "# sent = \"Admission to the Haskayne School of Business and OBHR 317.\"\n",
    "# sent = \"PHYS 211 or 221 or 227.\"\n",
    "# sent = \"MATH 277 and PHYS 259 and admission to a program in Engineering.\"\n",
    "# sent = \"PHYS 341; and 3 units from CPSC 217, 231 or DATA 211.\"\n",
    "# sent = \"ACSC 327; and MATH 323 or STAT 323.\"\n",
    "# sent = \"ANTH 203.\"s\n",
    "# sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"One of CORE 209, 435, KNES 355, NURS 303, 305, PSYC 203, 205, SOWK 300, 302, 304, 306, 363 or consent of the instructor(s).\"\n",
    "\n",
    "sent = replace_subject_code(sent)\n",
    "print(\"Original:\", sent)\n",
    "\n",
    "doc = expand_nlp(sent)\n",
    "print(\"Expand  :\", doc)\n",
    "\n",
    "\n",
    "doc = constituency_nlp(doc)\n",
    "print(\"Constituency:\", doc)\n",
    "\n",
    "print(doc.ents)\n",
    "print(doc._.replacements)\n",
    "print(doc._.json_logics)\n",
    "\n",
    "for key, replacement in doc._.replacements:\n",
    "    replacement: Span\n",
    "    print(key, replacement.text, replacement.ents)\n",
    "\n",
    "\n",
    "j = extract_doc(doc)\n",
    "pprint(j, indent=2, depth=10)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options={\"compact\": True, \"distance\": 100})\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"compact\": True, \"distance\": 100})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
