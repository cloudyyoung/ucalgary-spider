{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import SpanRuler, EntityRuler\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from nltk import Tree\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_generator():\n",
    "    yield from itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "letters = letter_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mongodb\n",
    "client = MongoClient(\"mongodb://root:password@localhost:27017/\")\n",
    "catalog = client.get_database(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by length of title\n",
    "subject_codes_docs = list(catalog.get_collection(\"subject_codes\").find())\n",
    "subject_codes_docs.sort(key=lambda x: len(x[\"title\"]), reverse=True)\n",
    "subject_codes_map = {doc[\"title\"]: doc[\"code\"] for doc in subject_codes_docs}\n",
    "subject_codes = [doc[\"code\"] for doc in subject_codes_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base patterns\n",
    "subject_code_regex = r\"([A-Z]{3,4})\"  # ART, MATH\n",
    "course_number_regex = r\"(\\d{2}-\\d|\\d{3}\\.\\d{1,2}|\\d{2,3})\"  # 101, 30-1, 599.45\n",
    "course_code_regex = rf\"{subject_code_regex} {course_number_regex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject_code(sentence: str, loose: bool=False):\n",
    "\tfor subject_code in subject_codes_docs:\n",
    "\t\tif loose:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]}\", rf\"{subject_code[\"code\"]}\", sentence)\n",
    "\t\telse:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]} {course_number_regex}\", rf\"{subject_code[\"code\"]} \\1\", sentence)\n",
    "\treturn sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_letter():\n",
    "    # Create an iterator that cycles through the alphabet\n",
    "    for letter in itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "        yield letter\n",
    "\n",
    "replacement_letters = get_replacement_letter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dict_to(target: dict, source: dict):\n",
    "    for key, value in source.items():\n",
    "        target[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension(\"course_code\", default=None, force=True)\n",
    "Doc.set_extension(\"replacements\", default=[], force=True)\n",
    "Doc.set_extension(\"json_logics\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "expand_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "constituency_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_custom_boundaries(doc)>"
      ]
     },
     "execution_count": 1283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc:\n",
    "        token.is_sent_start = False\n",
    "\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\":\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "            \n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"fix_ent_head\")\n",
    "def fix_ent_head(doc: Doc):\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "                ancestors = list(filter(lambda x: x.text in subject_codes, token.ancestors))\n",
    "                ancestor = ancestors[0] if len(ancestors) > 0 else None\n",
    "\n",
    "                if ancestor:\n",
    "                    token.head = ancestor\n",
    "                    token._.course_code = f\"{ancestor.text}{token.text}\"\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"expand_course_code\")\n",
    "def expand_course_code(doc: Doc):\n",
    "    sent = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in subject_codes:\n",
    "            continue\n",
    "\n",
    "        elif token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "            left_tokens = [token.head] + list(reversed(list(doc[: token.i])))\n",
    "\n",
    "            for left_token in left_tokens:\n",
    "                if left_token.text in subject_codes:\n",
    "                    sent += left_token.text_with_ws\n",
    "                    break\n",
    "\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "        else:\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc.ents = []\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "    {\n",
    "        \"label\": \"COURSE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": {\"REGEX\": subject_code_regex}},\n",
    "            {\"TEXT\": {\"REGEX\": course_number_regex}},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"REQUISITE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": \"RQ\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Z]\"}},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "entity_ruler.clear()\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "@Language.component(\"detect_entity\")\n",
    "def detect_entity(doc: Doc):\n",
    "    ents = entity_ruler.match(doc)\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"merge_entity_spans\")\n",
    "def merge_entity_spans(doc: Doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ is not None:\n",
    "                retokenizer.merge(ent, attrs={\"ENT_TYPE\": ent.label_, \"ENT_IOB\": \"B\", \"ENT_IOE\": \"E\", \"ENT_IOR\": \"\", \"pos\": \"PROPN\"})\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'fix_ent_head',\n",
       " 'expand_course_code',\n",
       " 'detect_entity',\n",
       " 'merge_entity_spans']"
      ]
     },
     "execution_count": 1288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_nlp.add_pipe(\"fix_ent_head\")\n",
    "expand_nlp.add_pipe(\"expand_course_code\")\n",
    "expand_nlp.add_pipe(\"detect_entity\")\n",
    "expand_nlp.add_pipe(\"merge_entity_spans\")\n",
    "\n",
    "expand_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeating_array(head: list, repeat_elemnts: list, repeat_times: int, tail: list):\n",
    "    repeated_part = repeat_elemnts * repeat_times\n",
    "    return head + repeated_part + tail\n",
    "\n",
    "def get_dynamic_patterns(head: list, repeat_tokens: list, repeat_range: range, tail: list):\n",
    "    patterns = []\n",
    "    for i in repeat_range:\n",
    "        patterns.append(get_repeating_array(head, repeat_tokens, i, tail))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replacement(key: str, replacements: list[tuple[str, Span]]):\n",
    "    if not replacements or not key:\n",
    "        return\n",
    "\n",
    "    key = key[:-1].strip() if key.strip().endswith(\".\") else key\n",
    "    \n",
    "    for _key, span in replacements:\n",
    "        if key == _key:\n",
    "            return span\n",
    "\n",
    "def find_json_logic(span: Span, json_logic: list[tuple[Span, dict]]):\n",
    "    if not json_logic or not span:\n",
    "        return\n",
    "    \n",
    "    for _span, logic in json_logic:\n",
    "        if span.text == _span.text:\n",
    "            return logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "### X Units of\n",
    "def x_units_of(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    units_required = int(span[0].text)\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"units\": {\n",
    "            \"required\": units_required,\n",
    "            \"from\": [{\"course\": course.text} for course in courses],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "x_units_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"IS_DIGIT\": True},\n",
    "        {\"LEMMA\": \"unit\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \"or\", \",\"]}},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\"X units of\", x_units_of_patterns, greedy=\"LONGEST\", on_match=x_units_of)\n",
    "### X Units of\n",
    "\n",
    "\n",
    "### X of\n",
    "def x_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    number_token = span[0]\n",
    "    switch = {\n",
    "        \"one\": 1,\n",
    "        \"two\": 2,\n",
    "        \"three\": 3,\n",
    "    }\n",
    "    number = switch.get(number_token.lemma_, 1)\n",
    "\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"courses\": {\n",
    "            \"required\": number,\n",
    "            \"from\": [{\"course\": course.text} for course in courses],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "x_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \",\"]}},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\"X of\", x_of_patterns, greedy=\"LONGEST\", on_match=x_of)\n",
    "### One of\n",
    "\n",
    "\n",
    "### Consent of\n",
    "def consent_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    consent_of = span[2:].text.strip()\n",
    "    if consent_of.endswith(\".\"):\n",
    "        consent_of = consent_of[:-1]\n",
    "    json_logic = {\"consent\": consent_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Consent of\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"consent\"},\n",
    "            {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Za-z, ]\"}, \"OP\": \"*\"},\n",
    "            {\"IS_SENT_START\": False},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=consent_of,\n",
    ")\n",
    "\n",
    "def admission_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    admission_of = span[2:].text.strip()\n",
    "    if admission_of.endswith(\".\"):\n",
    "        admission_of = admission_of[:-1]\n",
    "    json_logic = {\"admission\": admission_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "admission_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"LEMMA\": \"admission\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"NOT_IN\": [\"COURSE\"]}},\n",
    "    ],\n",
    "    range(1, 200),\n",
    "    [\n",
    "        {\"LEMMA\": {\"NOT_IN\": [\"and\", \"or\"]}, \"ENT_TYPE\": {\"NOT_IN\": [\"COURSE\"]}},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\n",
    "    \"Admission to\",\n",
    "    admission_of_patterns,\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=admission_of,\n",
    ")\n",
    "### Consent of\n",
    "\n",
    "\n",
    "### Both A and B\n",
    "def both_a_and_b(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    a = span[1]\n",
    "    b = span[3]\n",
    "    json_logic = {\"and\": [{\"course\": a.text}, {\"course\": b.text}]}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Both A and B\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"both\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "            {\"LEMMA\": \"and\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=both_a_and_b,\n",
    ")\n",
    "### Both A and B\n",
    "\n",
    "\n",
    "### Either A or B\n",
    "def either_a_or_b(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    a = span[1]\n",
    "    b = span[3]\n",
    "    json_logic = {\"or\": [{\"course\": a.text}, {\"course\": b.text}]}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "matcher.add(\n",
    "    \"Either A or B\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"either\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "            {\"LEMMA\": \"or\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=either_a_or_b,\n",
    ")\n",
    "### Either A or B\n",
    "\n",
    "\n",
    "### A, B, C, ..., and D\n",
    "def and_list(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    predicates = []\n",
    "\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "    for course in courses:\n",
    "        predicates.append({\"course\": course.text})\n",
    "\n",
    "    requisites = [ent for ent in span.ents if ent.label_ == \"REQUISITE\"]\n",
    "    for requisite in requisites:\n",
    "        requisite_span = find_replacement(requisite.text, doc._.replacements)\n",
    "        requisite_logic = find_json_logic(requisite_span, doc._.json_logics)\n",
    "        if requisite_logic:\n",
    "            predicates.append(requisite_logic)\n",
    "\n",
    "    json_logic = {\"and\": predicates}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "and_list_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \",\"]}},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \",\"]}},\n",
    "    ],\n",
    "    range(0, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    ")\n",
    "\n",
    "matcher.add(\"A, B, C, ..., and D\", and_list_patterns, greedy=\"LONGEST\", on_match=and_list)\n",
    "### A, B, C, ..., and D\n",
    "\n",
    "\n",
    "@Language.component(\"constitute_requisite\")\n",
    "def constitute_requisite(doc: Doc):\n",
    "    sent = doc.text\n",
    "    matches = matcher(doc)\n",
    "    replacements = []\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        letter = next(letters)\n",
    "        replacement = f\"RQ {letter}\"\n",
    "\n",
    "        span = doc[start:end]\n",
    "        replacements.append((replacement, span))\n",
    "\n",
    "        sent = re.sub(re.escape(span.text), replacement, sent)\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc._.replacements = replacements + doc._.replacements\n",
    "    new_doc._.json_logics = doc._.json_logics\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'constitute_requisite_1',\n",
       " 'detect_entity_1',\n",
       " 'merge_entity_spans_1',\n",
       " 'constitute_requisite_2',\n",
       " 'detect_entity_2',\n",
       " 'merge_entity_spans_2']"
      ]
     },
     "execution_count": 1292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constituency_nlp.add_pipe(\"constitute_requisite\", \"constitute_requisite_1\")\n",
    "constituency_nlp.add_pipe(\"detect_entity\", \"detect_entity_1\")\n",
    "constituency_nlp.add_pipe(\"merge_entity_spans\", \"merge_entity_spans_1\")\n",
    "constituency_nlp.add_pipe(\"constitute_requisite\", \"constitute_requisite_2\")\n",
    "constituency_nlp.add_pipe(\"detect_entity\", \"detect_entity_2\")\n",
    "constituency_nlp.add_pipe(\"merge_entity_spans\", \"merge_entity_spans_2\")\n",
    "\n",
    "constituency_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_format(tok):\n",
    "    # return \"_\".join([tok.orth_, tok.tag_])\n",
    "    return f\"{tok.orth_} ({tok.dep_})\"\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check_matcher = Matcher(nlp.vocab)\n",
    "sanity_check_matcher.add(\n",
    "    \"Format\",\n",
    "    [\n",
    "        [\n",
    "            {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "            {\"POS\": \"PUNCT\", \"OP\": \"*\"},\n",
    "            {\"DEP\": \"cc\"},\n",
    "            {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    ")\n",
    "\n",
    "def sanity_check(span: Span):\n",
    "    matches = sanity_check_matcher(span)\n",
    "    return len(matches) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_entity(token: Token, replacements: dict[str, Span], json_logics: list[tuple[Span, dict]]):\n",
    "    if token.ent_type_ == \"COURSE\":\n",
    "        return {\"course\": token.text}\n",
    "\n",
    "    elif token.ent_type_ == \"REQUISITE\":\n",
    "        replacement = find_replacement(token.text, replacements)\n",
    "        json_logic = find_json_logic(replacement, json_logics)\n",
    "        return json_logic\n",
    "\n",
    "def extract_doc(doc: Doc):\n",
    "    if not sanity_check(doc):\n",
    "        print(\"A - Sanity check failed\")\n",
    "        return None\n",
    "\n",
    "    logic_operator = None\n",
    "    predicates = []\n",
    "\n",
    "    # Find conjunction\n",
    "    for token in doc:\n",
    "        if token.lemma_ in [\"and\", \"or\"]:\n",
    "            if logic_operator is not None:\n",
    "                print(\"Multiple conjunctions found\")\n",
    "            else:\n",
    "                logic_operator = token.lemma_\n",
    "\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in [\"COURSE\", \"REQUISITE\"]:\n",
    "            print(token)\n",
    "            predicates.append(extract_entity(token, doc._.replacements, doc._.json_logics))\n",
    "\n",
    "    if len(predicates) == 1:\n",
    "        return predicates[0]\n",
    "\n",
    "    return {\n",
    "        logic_operator: predicates\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: HTST 300 and one of EAST 331, 333, HTST 209, 301, 315, 317, 405, 407.01, 407.02, 407.03, or consent of the Department.\n",
      "Expand  : HTST 300 and one of EAST 331, EAST 333, HTST 209, HTST 301, HTST 315, HTST 317, HTST 405, HTST 407.01, HTST 407.02, HTST 407.03, or consent of the Department.\n",
      "Constituency: RQ D, or RQ C\n",
      "RQ D -> HTST 300 and RQ A\n",
      "RQ A -> one of EAST 331, EAST 333, HTST 209, HTST 301, HTST 315, HTST 317, HTST 405, HTST 407.01, HTST 407.02, HTST 407.03\n",
      "RQ B -> EAST 331, EAST 333, HTST 209, HTST 301, HTST 315, HTST 317, HTST 405, HTST 407.01, HTST 407.02, HTST 407.03\n",
      "RQ C -> consent of the Department.\n",
      "one of EAST 331, EAST 333, HTST 209, HTST 301, HTST 315, HTST 317, HTST 405, HTST 407.01, HTST 407.02, HTST 407.03 -> {'courses': {'required': 1, 'from': [{'course': 'EAST 331'}, {'course': 'EAST 333'}, {'course': 'HTST 209'}, {'course': 'HTST 301'}, {'course': 'HTST 315'}, {'course': 'HTST 317'}, {'course': 'HTST 405'}, {'course': 'HTST 407.01'}, {'course': 'HTST 407.02'}, {'course': 'HTST 407.03'}]}}\n",
      "EAST 331, EAST 333, HTST 209, HTST 301, HTST 315, HTST 317, HTST 405, HTST 407.01, HTST 407.02, HTST 407.03 -> {'and': [{'course': 'EAST 331'}, {'course': 'EAST 333'}, {'course': 'HTST 209'}, {'course': 'HTST 301'}, {'course': 'HTST 315'}, {'course': 'HTST 317'}, {'course': 'HTST 405'}, {'course': 'HTST 407.01'}, {'course': 'HTST 407.02'}, {'course': 'HTST 407.03'}]}\n",
      "consent of the Department. -> {'consent': 'the Department'}\n",
      "HTST 300 and RQ A -> {'and': [{'course': 'HTST 300'}, {'courses': {'required': 1, 'from': [{'course': 'EAST 331'}, {'course': 'EAST 333'}, {'course': 'HTST 209'}, {'course': 'HTST 301'}, {'course': 'HTST 315'}, {'course': 'HTST 317'}, {'course': 'HTST 405'}, {'course': 'HTST 407.01'}, {'course': 'HTST 407.02'}, {'course': 'HTST 407.03'}]}}]}\n",
      "RQ D\n",
      "RQ C\n",
      "{ 'or': [ { 'and': [ {'course': 'HTST 300'},\n",
      "                     { 'courses': { 'from': [ {'course': 'EAST 331'},\n",
      "                                              {'course': 'EAST 333'},\n",
      "                                              {'course': 'HTST 209'},\n",
      "                                              {'course': 'HTST 301'},\n",
      "                                              {'course': 'HTST 315'},\n",
      "                                              {'course': 'HTST 317'},\n",
      "                                              {'course': 'HTST 405'},\n",
      "                                              {'course': 'HTST 407.01'},\n",
      "                                              {'course': 'HTST 407.02'},\n",
      "                                              {'course': 'HTST 407.03'}],\n",
      "                                    'required': 1}}]},\n",
      "          {'consent': 'the Department'}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RQ D\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">REQUISITE</span>\n",
       "</mark>\n",
       ", or \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RQ C\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">REQUISITE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cd1d54849b2e4fe9893eff8a25a56ad7-0\" class=\"displacy\" width=\"350\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">RQ D,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">or</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">RQ C</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd1d54849b2e4fe9893eff8a25a56ad7-0-0\" stroke-width=\"2px\" d=\"M62,102.0 62,85.33333333333333 147.0,85.33333333333333 147.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd1d54849b2e4fe9893eff8a25a56ad7-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M147.0,104.0 L151.0,96.0 143.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cd1d54849b2e4fe9893eff8a25a56ad7-0-1\" stroke-width=\"2px\" d=\"M62,102.0 62,68.66666666666666 250.0,68.66666666666666 250.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cd1d54849b2e4fe9893eff8a25a56ad7-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250.0,104.0 L254.0,96.0 246.0,96.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sent = \"Actuarial Science 327; Statistics 323; 3 units from Mathematics 311, 313, 367 or 375; and 3 units from Computer Science 217, 231, 235 or Data Science 211.\"\n",
    "# sent = \"CPSC 457 and 3 units from SENG 300, 301 or ENSF 480; and admission to the Schulich School of Engineering.\"\n",
    "# sent = \"SGMA 395 or ENTI 317 or 381.\"\n",
    "# sent = \"One of FILM 321 or 323 and one of FILM 331 or 333.\"\n",
    "# sent = \"FILM 331 or 333.\"\n",
    "# sent = \"ENCI 473; and ENGG 319 or ENDG 319.\"\n",
    "# sent = \"3 units from ENCI 481, ENEE 377 or 519.09.\"\n",
    "# sent = \"ENEL 341, BMEN 327 or ENGG 225.\"\n",
    "# sent = \"ENEL 471; and one of BMEN 319 or ENGG 319 or ENEL 419.\"\n",
    "# sent = \"3 units from ENGG 319, ENDG 319 or ENEL 419.\"\n",
    "# sent = \"FILM 201 and 3 units from 305 or 321.\"\n",
    "# sent = \"INDG 201 and 3 units from INDG 303 or 345.\"\n",
    "# sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"STAT 205 or 213; and admission to the Kinesiology Honours program; and consent of the Faculty.\"\n",
    "# sent = \"MATH 209 and admission to the Energy Engineering program.\"\n",
    "# sent = \"Both MATH 349 and 353; or both MATH 283 and 381; or MATH 267.\"\n",
    "# sent = \"MATH 431 or PMAT 431; MATH 429 or PMAT 429 or MATH 327 or PMAT 427.\"\n",
    "# sent = \"MATH 445 or 447; 3 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MATH 383; and 6 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MRSC 451 and consent of the Department.\"\n",
    "# sent = \"Admission to the Haskayne School of Business and OBHR 317.\"\n",
    "# sent = \"PHYS 211 or 221 or 227.\"\n",
    "# sent = \"MATH 277 and PHYS 259 and admission to a program in Engineering.\"\n",
    "# sent = \"PHYS 341; and 3 units from CPSC 217, 231 or DATA 211.\"\n",
    "# sent = \"ACSC 327; and MATH 323 or STAT 323.\"\n",
    "# sent = \"ANTH 203.\"s\n",
    "# sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"One of CORE 209, 435, KNES 355, NURS 303, 305, PSYC 203, 205, SOWK 300, 302, 304, 306, 363 or consent of the instructor(s).\"\n",
    "\n",
    "# Problematic sentences\n",
    "sent = \"History 300 and one of East Asian Studies 331, 333, History 209, 301, 315, 317, 405, 407.01, 407.02, 407.03, or consent of the Department.\"\n",
    "# sent = \"Kinesiology 203, 213, 323 and admission to the Faculty of Kinesiology.\"\n",
    "# sent = \"Mathematics 30-1, Mathematics 30-2, or Mathematics 31.\"\n",
    "# sent = \"Admission to the Psychology major or Honours program and Psychology 300, 301, 369.\"\n",
    "# sent = \"Computer Science 219, 233 or Data Science 311 and enrolment in one of the Majors in Computer Science, Bioinformatics, Electrical Engineering, Software Engineering, Computer Engineering, Natural Sciences with a primary concentration in Computer Science.\"\n",
    "# sent = \"Computer Science 219, 233 or Data Science 311 and enrolment in one of the Majors in Computer Science, Bioinformatics, Electrical Engineering, Software Engineering, Computer Engineering, Natural Sciences with a primary concentration in Computer Science.\"\n",
    "\n",
    "\n",
    "sent = replace_subject_code(sent)\n",
    "print(\"Original:\", sent)\n",
    "\n",
    "\n",
    "doc = expand_nlp(sent)\n",
    "print(\"Expand  :\", doc)\n",
    "\n",
    "\n",
    "doc = constituency_nlp(doc)\n",
    "print(\"Constituency:\", doc)\n",
    "\n",
    "\n",
    "for key, replacement in doc._.replacements:\n",
    "    replacement: Span\n",
    "    print(f\"{key} -> {replacement}\")\n",
    "\n",
    "for key, json_logic in doc._.json_logics:\n",
    "    json_logic: dict\n",
    "    print(f\"{key} -> {json_logic}\")\n",
    "\n",
    "\n",
    "j = extract_doc(doc)\n",
    "pprint(j, indent=2, depth=10)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options={\"compact\": True, \"distance\": 100})\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"compact\": True, \"distance\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_nlp(course: dict, sent: str):\n",
    "    sent = replace_subject_code(sent)\n",
    "    doc = expand_nlp(sent)\n",
    "    doc = constituency_nlp(doc)\n",
    "\n",
    "    # If doc is not consist of only COURSE, REQUISITE, and CONJUNCTION\n",
    "    for token in doc:\n",
    "        is_token_entity = token.ent_type_ in [\"COURSE\", \"REQUISITE\"]\n",
    "        is_token_conjunction = token.text.lower() in [\"and\", \"or\"]\n",
    "        is_token_puctuation = token.is_punct or token.is_space\n",
    "        if not is_token_entity and not is_token_conjunction and not is_token_puctuation:\n",
    "            return None\n",
    "        \n",
    "    # If all conjunctions are not the same\n",
    "    all_and = all(conj.text == \"and\" for conj in doc if conj.ent_type_ == \"CONJUNCTION\")\n",
    "    all_or = all(conj.text == \"or\" for conj in doc if conj.ent_type_ == \"CONJUNCTION\")\n",
    "    if not all_and and not all_or:\n",
    "        return None\n",
    "\n",
    "    j = extract_doc(doc)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all courses\n",
    "# courses = list(\n",
    "#     catalog.get_collection(\"courses\").find(\n",
    "#         {\"prereq\": {\"$ne\": None}, \"career\": \"Undergraduate Programs\", \"active\": True}\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# courses_prereq = catalog.get_collection(\"courses_prereq\")\n",
    "# courses_prereq.delete_many({})\n",
    "\n",
    "# for course in courses:\n",
    "#     prereq = course[\"prereq\"]\n",
    "\n",
    "#     if prereq:\n",
    "#         print(prereq)\n",
    "\n",
    "#         result = try_nlp(course, prereq)\n",
    "\n",
    "#         print(result)\n",
    "#         print(\"\")\n",
    "\n",
    "#         courses_prereq.insert_one(\n",
    "#             {\"course\": course[\"code\"], \"prereq_text\": prereq, \"prereq\": result}\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
