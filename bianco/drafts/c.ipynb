{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.pipeline import SpanRuler, EntityRuler\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from nltk import Tree\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2834,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_generator():\n",
    "    yield from itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "\n",
    "letters = letter_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2835,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mongodb\n",
    "client = MongoClient(\"mongodb://root:password@localhost:27017/\")\n",
    "catalog = client.get_database(\"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2836,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by length of title\n",
    "subject_codes_docs = list(catalog.get_collection(\"subject_codes\").find())\n",
    "subject_codes_docs.sort(key=lambda x: len(x[\"title\"]), reverse=True)\n",
    "subject_codes_map = {doc[\"title\"]: doc[\"code\"] for doc in subject_codes_docs}\n",
    "subject_codes = [doc[\"code\"] for doc in subject_codes_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2837,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base patterns\n",
    "subject_code_regex = r\"([A-Z]{3,4})\"  # ART, MATH\n",
    "course_number_regex = r\"(\\d{2}-\\d|\\d{3}\\.\\d{1,2}|\\d{2,3})\"  # 101, 30-1, 599.45\n",
    "course_code_regex = rf\"{subject_code_regex} {course_number_regex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2838,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_subject_code(sentence: str, loose: bool=False):\n",
    "\tfor subject_code in subject_codes_docs:\n",
    "\t\tif loose:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]}\", rf\"{subject_code[\"code\"]}\", sentence)\n",
    "\t\telse:\n",
    "\t\t\tsentence = re.sub(rf\"{subject_code[\"title\"]} {course_number_regex}\", rf\"{subject_code[\"code\"]} \\1\", sentence)\n",
    "\treturn sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2839,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_letter():\n",
    "    # Create an iterator that cycles through the alphabet\n",
    "    for letter in itertools.cycle(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"):\n",
    "        yield letter\n",
    "\n",
    "replacement_letters = get_replacement_letter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2840,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension(\"course_code\", default=None, force=True)\n",
    "Doc.set_extension(\"replacements\", default=[], force=True)\n",
    "Doc.set_extension(\"json_logics\", default=[], force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2841,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "expand_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "constituency_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "structure_nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.merge_course_number(doc)>"
      ]
     },
     "execution_count": 2842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_number_matcher = Matcher(nlp.vocab)\n",
    "course_number_matcher.add(\n",
    "    \"COURSE_NUMBER\",\n",
    "    [\n",
    "        [\n",
    "            {\"IS_DIGIT\": True, \"LENGTH\": {\">=\": 2, \"<=\": 3}},\n",
    "            {\"TEXT\": {\"IN\": [\"-\", \".\"]}},\n",
    "            {\"IS_DIGIT\": True, \"LENGTH\": {\">=\": 1, \"<=\": 2}},\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "\n",
    "@Language.component(\"merge_course_number\")\n",
    "def merge_course_number(doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for match_id, start, end in course_number_matcher(doc):\n",
    "            retokenizer.merge(doc[start:end])\n",
    "\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"merge_course_number\", after=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2843,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"fix_ent_head\")\n",
    "def fix_ent_head(doc: Doc):\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "                ancestors = list(filter(lambda x: x.text in subject_codes, token.ancestors))\n",
    "                ancestor = ancestors[0] if len(ancestors) > 0 else None\n",
    "\n",
    "                if ancestor:\n",
    "                    token.head = ancestor\n",
    "                    token._.course_code = f\"{ancestor.text}{token.text}\"\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2844,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"expand_course_code\")\n",
    "def expand_course_code(doc: Doc):\n",
    "    sent = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text in subject_codes:\n",
    "            continue\n",
    "\n",
    "        elif token.pos_ == \"NUM\" and re.match(course_number_regex, token.text) is not None:\n",
    "            left_tokens = [token.head] + list(reversed(list(doc[: token.i])))\n",
    "\n",
    "            for left_token in left_tokens:\n",
    "                if left_token.text in subject_codes:\n",
    "                    sent += left_token.text_with_ws\n",
    "                    break\n",
    "\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "        else:\n",
    "            sent += token.text_with_ws\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc.ents = []\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2845,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ruler = EntityRuler(nlp)\n",
    "patterns = [\n",
    "    {\n",
    "        \"label\": \"COURSE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": {\"REGEX\": subject_code_regex}},\n",
    "            {\"POS\": \"NUM\"}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"REQUISITE\",\n",
    "        \"pattern\": [\n",
    "            {\"TEXT\": \"RQ\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Z]\"}},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "entity_ruler.clear()\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "@Language.component(\"detect_entity\")\n",
    "def detect_entity(doc: Doc):\n",
    "    ents = entity_ruler.match(doc)\n",
    "    doc.ents = ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2846,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"merge_entity_spans\")\n",
    "def merge_entity_spans(doc: Doc):\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ is not None:\n",
    "                retokenizer.merge(ent, attrs={\"ENT_TYPE\": ent.label_, \"ENT_IOB\": \"B\", \"ENT_IOE\": \"E\", \"ENT_IOR\": \"\", \"pos\": \"PROPN\"})\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'fix_ent_head',\n",
       " 'expand_course_code',\n",
       " 'detect_entity',\n",
       " 'merge_entity_spans']"
      ]
     },
     "execution_count": 2847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_nlp.add_pipe(\"fix_ent_head\")\n",
    "expand_nlp.add_pipe(\"expand_course_code\")\n",
    "expand_nlp.add_pipe(\"detect_entity\")\n",
    "expand_nlp.add_pipe(\"merge_entity_spans\")\n",
    "\n",
    "expand_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2848,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeating_array(head: list, repeat_elemnts: list, repeat_times: int, tail: list):\n",
    "    repeated_part = repeat_elemnts * repeat_times\n",
    "    return head + repeated_part + tail\n",
    "\n",
    "def get_dynamic_patterns(head: list, repeat_tokens: list, repeat_range: range, tail: list):\n",
    "    patterns = []\n",
    "    for i in repeat_range:\n",
    "        patterns.append(get_repeating_array(head, repeat_tokens, i, tail))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_replacement(key: str, replacements: list[tuple[str, Span]]):\n",
    "    if not replacements or not key:\n",
    "        return\n",
    "\n",
    "    key = key[:-1].strip() if key.strip().endswith(\".\") else key\n",
    "    \n",
    "    for _key, span in replacements:\n",
    "        if key == _key:\n",
    "            return span\n",
    "\n",
    "def find_json_logic(span: Span, json_logic: list[tuple[Span, dict]]):\n",
    "    if not json_logic or not span:\n",
    "        return\n",
    "    \n",
    "    for _span, logic in json_logic:\n",
    "        if span.text == _span.text:\n",
    "            return logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2850,
   "metadata": {},
   "outputs": [],
   "source": [
    "requisite_pattern_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "### X Units of\n",
    "def x_units_of(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    units_required = int(span[0].text)\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"units\": {\n",
    "            \"required\": units_required,\n",
    "            \"from\": [{\"course\": course.text} for course in courses],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "x_units_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"IS_DIGIT\": True},\n",
    "        {\"LEMMA\": \"unit\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \",\"]}, \"OP\": \"{1,2}\"},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "requisite_pattern_matcher.add(\"X units of\", x_units_of_patterns, greedy=\"LONGEST\", on_match=x_units_of)\n",
    "### X Units of\n",
    "\n",
    "\n",
    "def x_units(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    units_required = int(span[0].text)\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"units\": {\n",
    "            \"required\": units_required,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "requisite_pattern_matcher.add(\"x_units\", [[{\"IS_DIGIT\": True}, {\"LEMMA\": \"unit\"}]], greedy=\"LONGEST\", on_match=x_units)\n",
    "\n",
    "### X of\n",
    "def x_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "\n",
    "    number_token = span[0]\n",
    "    switch = {\n",
    "        \"one\": 1,\n",
    "        \"two\": 2,\n",
    "        \"three\": 3,\n",
    "    }\n",
    "    number = switch.get(number_token.lemma_, 1)\n",
    "\n",
    "    courses = [ent for ent in span.ents if ent.label_ == \"COURSE\"]\n",
    "\n",
    "    json_logic = {\n",
    "        \"courses\": {\n",
    "            \"required\": number,\n",
    "            \"from\": [{\"course\": course.text} for course in courses],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "x_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"POS\": \"NUM\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \",\"]}},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": \"COURSE\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "requisite_pattern_matcher.add(\"X of\", x_of_patterns, greedy=\"LONGEST\", on_match=x_of)\n",
    "### One of\n",
    "\n",
    "\n",
    "### Consent of\n",
    "def consent_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    consent_of = span[2:].text.strip()\n",
    "    if consent_of.endswith(\".\"):\n",
    "        consent_of = consent_of[:-1]\n",
    "    json_logic = {\"consent\": consent_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "requisite_pattern_matcher.add(\n",
    "    \"Consent of\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"consent\"},\n",
    "            {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "            {\"TEXT\": {\"REGEX\": \"[A-Za-z, ]\"}, \"OP\": \"*\"},\n",
    "            {\"IS_SENT_START\": False},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=consent_of,\n",
    ")\n",
    "\n",
    "def admission_of(matcher, doc: Doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    admission_of = span[2:].text.strip()\n",
    "    if admission_of.endswith(\".\"):\n",
    "        admission_of = admission_of[:-1]\n",
    "    json_logic = {\"admission\": admission_of}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "admission_of_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"LEMMA\": \"admission\"},\n",
    "        {\"POS\": \"ADP\", \"OP\": \"+\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"NOT_IN\": [\"COURSE\"]}},\n",
    "    ],\n",
    "    range(1, 200),\n",
    "    [\n",
    "        {\"LEMMA\": {\"NOT_IN\": [\"and\", \"or\", \",\", \";\"]}, \"ENT_TYPE\": {\"NOT_IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    ")\n",
    "\n",
    "requisite_pattern_matcher.add(\n",
    "    \"Admission to\",\n",
    "    admission_of_patterns,\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=admission_of,\n",
    ")\n",
    "### Consent of\n",
    "\n",
    "\n",
    "### Both A and B\n",
    "def both_a_and_b(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    a = span[1]\n",
    "    b = span[3]\n",
    "    json_logic = {\"and\": [{\"course\": a.text}, {\"course\": b.text}]}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "requisite_pattern_matcher.add(\n",
    "    \"Both A and B\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"both\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "            {\"LEMMA\": \"and\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=both_a_and_b,\n",
    ")\n",
    "### Both A and B\n",
    "\n",
    "\n",
    "### Either A or B\n",
    "def either_a_or_b(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    a = span[1]\n",
    "    b = span[3]\n",
    "    json_logic = {\"or\": [{\"course\": a.text}, {\"course\": b.text}]}\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "requisite_pattern_matcher.add(\n",
    "    \"Either A or B\",\n",
    "    [\n",
    "        [\n",
    "            {\"LEMMA\": \"either\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "            {\"LEMMA\": \"or\"},\n",
    "            {\"ENT_TYPE\": \"COURSE\"},\n",
    "        ]\n",
    "    ],\n",
    "    greedy=\"LONGEST\",\n",
    "    on_match=either_a_or_b,\n",
    ")\n",
    "### Either A or B\n",
    "\n",
    "@Language.component(\"constitute_requisite\")\n",
    "def constitute_requisite(doc: Doc):\n",
    "    sent = doc.text\n",
    "    matches = requisite_pattern_matcher(doc)\n",
    "    replacements = []\n",
    "\n",
    "    # sort matches by length of span\n",
    "    matches = sorted(matches, key=lambda x: x[2] - x[1], reverse=True)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        letter = next(letters)\n",
    "        replacement = f\"RQ {letter}\"\n",
    "        span = doc[start:end]\n",
    "        new_sent = re.sub(re.escape(span.text), replacement, sent)\n",
    "\n",
    "        if new_sent != sent:\n",
    "            sent = new_sent\n",
    "            replacements.append((replacement, span))\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc._.replacements = doc._.replacements + replacements\n",
    "    new_doc._.json_logics = doc._.json_logics\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'constitute_requisite_1',\n",
       " 'detect_entity_1',\n",
       " 'merge_entity_spans_1']"
      ]
     },
     "execution_count": 2851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(1, 2):\n",
    "    constituency_nlp.add_pipe(\"constitute_requisite\", f\"constitute_requisite_{t}\")\n",
    "    constituency_nlp.add_pipe(\"detect_entity\", f\"detect_entity_{t}\")\n",
    "    constituency_nlp.add_pipe(\"merge_entity_spans\", f\"merge_entity_spans_{t}\")\n",
    "\n",
    "constituency_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2852,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_minor_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "### A, B, C, ..., and D\n",
    "def and_list(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    predicates = []\n",
    "\n",
    "    for ent in span.ents:\n",
    "        if ent.label_ == \"COURSE\":\n",
    "            predicates.append({\"course\": ent.text})\n",
    "\n",
    "        elif ent.label_ == \"REQUISITE\":\n",
    "            requisite_span = find_replacement(ent.text, doc._.replacements)\n",
    "            requisite_logic = find_json_logic(requisite_span, doc._.json_logics)\n",
    "            if requisite_logic:\n",
    "                predicates.append(requisite_logic)\n",
    "\n",
    "\n",
    "    if len(predicates) == 1:\n",
    "        json_logic = predicates[0]\n",
    "    else:\n",
    "        json_logic = {\"and\": predicates}\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "and_list_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    "    [\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \",\"]}},\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    "    range(0, 20),\n",
    "    [\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \",\"]}},\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\",\"]}, \"OP\": \"?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "structure_minor_matcher.add(\"A, B, C, ..., and D\", and_list_patterns, greedy=\"LONGEST\", on_match=and_list)\n",
    "### A, B, C, ..., and D\n",
    "\n",
    "\n",
    "### A, B, C, ..., or D\n",
    "def or_list(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    predicates = []\n",
    "    \n",
    "    for ent in span.ents:\n",
    "        if ent.label_ == \"COURSE\":\n",
    "            predicates.append({\"course\": ent.text})\n",
    "\n",
    "        elif ent.label_ == \"REQUISITE\":\n",
    "            requisite_span = find_replacement(ent.text, doc._.replacements)\n",
    "            requisite_logic = find_json_logic(requisite_span, doc._.json_logics)\n",
    "            if requisite_logic:\n",
    "                predicates.append(requisite_logic)\n",
    "\n",
    "    if len(predicates) == 1:\n",
    "        json_logic = predicates[0]\n",
    "    else:\n",
    "        json_logic = {\"or\": predicates}\n",
    "\n",
    "    doc._.json_logics.append((span, json_logic))\n",
    "\n",
    "or_list_patterns = get_dynamic_patterns(\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    "    [\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \",\"]}},\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    "    range(0, 20),\n",
    "    [\n",
    "        {\"TEXT\": {\"IN\": [\"or\"]}},\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\",\"]}, \"OP\": \"?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "structure_minor_matcher.add(\"A, B, C, ..., or D\", or_list_patterns, greedy=\"LONGEST\", on_match=or_list)\n",
    "### A, B, C, ..., or D\n",
    "\n",
    "\n",
    "@Language.component(\"constitute_structure_minor\")\n",
    "def constitute_structure(doc: Doc):\n",
    "    sent = doc.text\n",
    "    matches = structure_minor_matcher(doc)\n",
    "    replacements = []\n",
    "\n",
    "    # sort matches by length of span\n",
    "    matches = sorted(matches, key=lambda x: x[2] - x[1], reverse=True)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        letter = next(letters)\n",
    "        replacement = f\"RQ {letter}\"\n",
    "        span = doc[start:end]\n",
    "        new_sent = re.sub(re.escape(span.text), replacement, sent)\n",
    "\n",
    "        if new_sent != sent:\n",
    "            sent = new_sent\n",
    "            replacements.append((replacement, span))\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc._.replacements = doc._.replacements + replacements\n",
    "    new_doc._.json_logics = doc._.json_logics\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2853,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_major_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "### A; and B; and C; ... and D\n",
    "major_and_list_patterns = get_dynamic_patterns(\n",
    "    [],\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\"and\", \";\"]}, \"OP\": \"+\"},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    ")\n",
    "structure_major_matcher.add(\"A; and B; and C; ... and D\", major_and_list_patterns, greedy=\"LONGEST\", on_match=and_list)\n",
    "### A; and B; and C; ... and D\n",
    "\n",
    "\n",
    "### A; or B; or C; ... or D\n",
    "major_or_list_patterns = get_dynamic_patterns(\n",
    "    [],\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "        {\"TEXT\": {\"IN\": [\"or\", \";\"]}, \"OP\": \"+\"},\n",
    "    ],\n",
    "    range(1, 20),\n",
    "    [\n",
    "        {\"ENT_TYPE\": {\"IN\": [\"COURSE\", \"REQUISITE\"]}},\n",
    "    ],\n",
    ")\n",
    "structure_major_matcher.add(\"A; or B; or C; ... or D\", major_or_list_patterns, greedy=\"LONGEST\", on_match=or_list)\n",
    "### A; or B; or C; ... or D\n",
    "\n",
    "@Language.component(\"constitute_structure_major\")\n",
    "def constitute_structure(doc: Doc):\n",
    "    sent = doc.text\n",
    "    matches = structure_major_matcher(doc)\n",
    "    replacements = []\n",
    "\n",
    "    # sort matches by length of span\n",
    "    matches = sorted(matches, key=lambda x: x[2] - x[1], reverse=True)\n",
    "\n",
    "    for match_id, start, end in matches:\n",
    "        letter = next(letters)\n",
    "        replacement = f\"RQ {letter}\"\n",
    "        span = doc[start:end]\n",
    "        new_sent = re.sub(re.escape(span.text), replacement, sent)\n",
    "        \n",
    "        if new_sent != sent:\n",
    "            sent = new_sent\n",
    "            replacements.append((replacement, span))\n",
    "\n",
    "    new_doc = nlp(sent)\n",
    "    new_doc._.replacements = doc._.replacements + replacements\n",
    "    new_doc._.json_logics = doc._.json_logics\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2854,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'constitute_structure_minor_1',\n",
       " 'detect_entity_1',\n",
       " 'merge_entity_spans_1',\n",
       " 'constitute_structure_minor_2',\n",
       " 'detect_entity_2',\n",
       " 'merge_entity_spans_2',\n",
       " 'constitute_structure_minor_3',\n",
       " 'detect_entity_3',\n",
       " 'merge_entity_spans_3',\n",
       " 'constitute_structure_minor_4',\n",
       " 'detect_entity_4',\n",
       " 'merge_entity_spans_4',\n",
       " 'constitute_structure_major_6',\n",
       " 'detect_entity_6',\n",
       " 'merge_entity_spans_6',\n",
       " 'constitute_structure_major_7',\n",
       " 'detect_entity_7',\n",
       " 'merge_entity_spans_7',\n",
       " 'constitute_structure_major_8',\n",
       " 'detect_entity_8',\n",
       " 'merge_entity_spans_8',\n",
       " 'constitute_structure_major_9',\n",
       " 'detect_entity_9',\n",
       " 'merge_entity_spans_9']"
      ]
     },
     "execution_count": 2854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in range(1, 5):\n",
    "    structure_nlp.add_pipe(\"constitute_structure_minor\", f\"constitute_structure_minor_{t}\")\n",
    "    structure_nlp.add_pipe(\"detect_entity\", f\"detect_entity_{t}\")\n",
    "    structure_nlp.add_pipe(\"merge_entity_spans\", f\"merge_entity_spans_{t}\")\n",
    "\n",
    "for t in range(6, 10):\n",
    "    structure_nlp.add_pipe(\"constitute_structure_major\", f\"constitute_structure_major_{t}\")\n",
    "    structure_nlp.add_pipe(\"detect_entity\", f\"detect_entity_{t}\")\n",
    "    structure_nlp.add_pipe(\"merge_entity_spans\", f\"merge_entity_spans_{t}\")\n",
    "    \n",
    "structure_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2855,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_format(tok):\n",
    "    # return \"_\".join([tok.orth_, tok.tag_])\n",
    "    return f\"{tok.orth_} ({tok.dep_})\"\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2856,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(span: Span):\n",
    "    if len(span) == 0:\n",
    "        return False\n",
    "\n",
    "    token = span[0]\n",
    "    is_ent_type = token.ent_type_ in [\"COURSE\", \"REQUISITE\"]\n",
    "\n",
    "    if len(span) == 1 and is_ent_type:\n",
    "        return True\n",
    "\n",
    "    if len(span) == 2 and is_ent_type:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity(token: Token, replacements: dict[str, Span], json_logics: list[tuple[Span, dict]]):\n",
    "    if token.ent_type_ == \"COURSE\":\n",
    "        return {\"course\": token.text}\n",
    "\n",
    "    elif token.ent_type_ == \"REQUISITE\":\n",
    "        replacement = find_replacement(token.text, replacements)\n",
    "        json_logic = find_json_logic(replacement, json_logics)\n",
    "        return json_logic\n",
    "\n",
    "def extract_doc(doc: Doc):\n",
    "    if not sanity_check(doc):\n",
    "        print(\"A - Sanity check failed\")\n",
    "        return None\n",
    "\n",
    "    token = doc[0]\n",
    "    return extract_entity(token, doc._.replacements, doc._.json_logics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ENSF 300; and 3 units from ENGG 319, ENDG 319 or ENEL 419; and 3 units from ENSF 337, ENCM 335, 339, or ENGO 333.\n",
      "Expand  : ENSF 300; and 3 units from ENGG 319, ENDG 319 or ENEL 419; and 3 units from ENSF 337, ENCM 335, ENSF 339, or ENGO 333.\n",
      "Constituency: ENSF 300; and RQ C; and RQ B.\n",
      "Structure: RQ F\n",
      "\n",
      "RQ B -> 3 units from ENSF 337, ENCM 335, ENSF 339, or ENGO 333\n",
      "RQ C -> 3 units from ENGG 319, ENDG 319 or ENEL 419\n",
      "RQ F -> ENSF 300; and RQ C; and RQ B.\n",
      "\n",
      "3 units -> {'units': {'required': 3}}\n",
      "3 units -> {'units': {'required': 3}}\n",
      "3 units from ENSF 337, ENCM 335, ENSF 339, or ENGO 333 -> {'units': {'required': 3, 'from': [{'course': 'ENSF 337'}, {'course': 'ENCM 335'}, {'course': 'ENSF 339'}, {'course': 'ENGO 333'}]}}\n",
      "3 units from ENGG 319, ENDG 319 or ENEL 419 -> {'units': {'required': 3, 'from': [{'course': 'ENGG 319'}, {'course': 'ENDG 319'}, {'course': 'ENEL 419'}]}}\n",
      "ENSF 300; and RQ C; and RQ B. -> {'and': [{'course': 'ENSF 300'}, {'units': {'required': 3, 'from': [{'course': 'ENGG 319'}, {'course': 'ENDG 319'}, {'course': 'ENEL 419'}]}}, {'units': {'required': 3, 'from': [{'course': 'ENSF 337'}, {'course': 'ENCM 335'}, {'course': 'ENSF 339'}, {'course': 'ENGO 333'}]}}]}\n",
      "\n",
      "{ 'and': [ {'course': 'ENSF 300'},\n",
      "           { 'units': { 'from': [ {'course': 'ENGG 319'},\n",
      "                                  {'course': 'ENDG 319'},\n",
      "                                  {'course': 'ENEL 419'}],\n",
      "                        'required': 3}},\n",
      "           { 'units': { 'from': [ {'course': 'ENSF 337'},\n",
      "                                  {'course': 'ENCM 335'},\n",
      "                                  {'course': 'ENSF 339'},\n",
      "                                  {'course': 'ENGO 333'}],\n",
      "                        'required': 3}}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    RQ F\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">REQUISITE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9ec71cb1d813433dba4b760b5369442e-0\" class=\"displacy\" width=\"150\" height=\"137.0\" direction=\"ltr\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">RQ F</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sent = \"Actuarial Science 327; Statistics 323; 3 units from Mathematics 311, 313, 367 or 375; and 3 units from Computer Science 217, 231, 235 or Data Science 211.\"\n",
    "# sent = \"CPSC 457 and 3 units from SENG 300, 301 or ENSF 480; and admission to the Schulich School of Engineering.\"\n",
    "# sent = \"SGMA 395 or ENTI 317 or 381.\"\n",
    "# sent = \"One of FILM 321 or 323 and one of FILM 331 or 333.\"\n",
    "# sent = \"FILM 331 or 333.\"\n",
    "# sent = \"ENCI 473; and ENGG 319 or ENDG 319.\"\n",
    "# sent = \"3 units from ENCI 481, ENEE 377 or 519.09.\"\n",
    "# sent = \"ENEL 341, BMEN 327 or ENGG 225.\"\n",
    "# sent = \"ENEL 471; and one of BMEN 319 or ENGG 319 or ENEL 419.\"\n",
    "# sent = \"3 units from ENGG 319, ENDG 319 or ENEL 419.\"\n",
    "# sent = \"FILM 201 and 3 units from 305 or 321.\"\n",
    "# sent = \"INDG 201 and 3 units from INDG 303 or 345.\"\n",
    "# sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"STAT 205 or 213; and admission to the Kinesiology Honours program; and consent of the Faculty.\"\n",
    "# sent = \"MATH 209 and admission to the Energy Engineering program.\"\n",
    "# sent = \"Both MATH 349 and 353; or both MATH 283 and 381; or MATH 267.\"\n",
    "# sent = \"MATH 431 or PMAT 431; MATH 429 or PMAT 429 or MATH 327 or PMAT 427.\"\n",
    "# sent = \"MATH 445 or 447; 3 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MATH 383; and 6 units of Mathematics in the Field of Mathematics at the 400 level or above.\"\n",
    "# sent = \"MRSC 451 and consent of the Department.\"\n",
    "# sent = \"Admission to the Haskayne School of Business and OBHR 317.\"\n",
    "# sent = \"PHYS 211 or 221 or 227.\"\n",
    "# sent = \"MATH 277 and PHYS 259 and admission to a program in Engineering.\"\n",
    "# sent = \"PHYS 341; and 3 units from CPSC 217, 231 or DATA 211.\"\n",
    "# sent = \"ACSC 327; and MATH 323 or STAT 323.\"\n",
    "# sent = \"ANTH 203.\"\n",
    "sent = \"One of GEOG 211, 251, 253, UBST 253, GLGY 201, 209; and consent of the Department.\"\n",
    "# sent = \"One of CORE 209, 435, KNES 355, NURS 303, 305, PSYC 203, 205, SOWK 300, 302, 304, 306, 363 or consent of the instructor(s).\"\n",
    "\n",
    "# Problematic sentences\n",
    "# sent = \"History 300 and one of East Asian Studies 331, 333, History 209, 301, 315, 317, 405, 407.01, 407.02, 407.03, or consent of the Department.\"\n",
    "# sent = \"Kinesiology 203, 213, 323 and admission to the Faculty of Kinesiology.\"\n",
    "# sent = \"Mathematics 30-1, Mathematics 30-2, or Mathematics 31.\"\n",
    "# sent = \"Admission to the Psychology major or Honours program and Psychology 300, 301, 369.\"\n",
    "# sent = \"Computer Science 219, 233 or Data Science 311 and enrolment in one of the Majors in Computer Science, Bioinformatics, Electrical Engineering, Software Engineering, Computer Engineering, Natural Sciences with a primary concentration in Computer Science.\"\n",
    "# sent = \"Computer Science 219, 233 or Data Science 311 and enrolment in one of the Majors in Computer Science, Bioinformatics, Electrical Engineering, Software Engineering, Computer Engineering, Natural Sciences with a primary concentration in Computer Science.\"\n",
    "# sent = \"Admission to the Haskayne School of Business, and 54 units including Accounting 217.\"\n",
    "# sent = \"3 units from Engineering 204, Chemistry 201, 209 or 211; and Chemistry 203 or 213; and 3 units from Mathematics 249, 265, 275.\"\n",
    "# sent = \"Software Engineering for Engineers 300; and 3 units from Engineering 319, Digital Engineering 319 or Electrical Engineering 419; and 3 units from Software Engineering for Engineers 337, Computer Engineering 335, 339, or Geomatics Engineering 333.\"\n",
    "# sent = \"3 units from Computer Science 219, 233 or 235; and Computer Science 251 or Statistics 213; and Mathematics 271 or 273; and 3 units from Mathematics 249, 265 or 275; and Philosophy 279 or 377.\"\n",
    "# sent = \"Psychology 200, 201, and admission to the International Indigenous Studies major.\"\n",
    "sent = \"Geology 201 and 202; and Mathematics 267 or 277; and Physics 211 or 221, and 223.\"\n",
    "sent = \"Physics 481; and Mathematics 433, Physics 435 or Physics Engineering 435.\"\n",
    "sent = \"Chemistry 333 or 433; and one of 353 or 355; or Chemistry 357 and 409.\"\n",
    "sent = \"Software Engineering for Engineers 300; and 3 units from Engineering 319, Digital Engineering 319 or Electrical Engineering 419; and 3 units from Software Engineering for Engineers 337, Computer Engineering 335, 339, or Geomatics Engineering 333.\"\n",
    "# sent = \"Medical Science 301 or 401: and Statistics 321.\"\n",
    "# sent = \"Biology 30 or 212; Chemistry 30 or 212; and Mathematics 30-1 or 212.\"\n",
    "# sent = \"60 units.\"\n",
    "\n",
    "\n",
    "sent = replace_subject_code(sent)\n",
    "print(\"Original:\", sent)\n",
    "\n",
    "\n",
    "doc = expand_nlp(sent)\n",
    "print(\"Expand  :\", doc)\n",
    "\n",
    "\n",
    "doc = constituency_nlp(doc)\n",
    "print(\"Constituency:\", doc)\n",
    "\n",
    "doc = structure_nlp(doc)\n",
    "print(\"Structure:\", doc)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for key, replacement in doc._.replacements:\n",
    "    replacement: Span\n",
    "    print(f\"{key} -> {replacement}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "for key, json_logic in doc._.json_logics:\n",
    "    json_logic: dict\n",
    "    print(f\"{key} -> {json_logic}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "j = extract_doc(doc)\n",
    "pprint(j, indent=2, depth=10)\n",
    "\n",
    "\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options={\"compact\": True, \"distance\": 100})\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={\"compact\": True, \"distance\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_nlp(course: dict, sent: str):\n",
    "    sent = replace_subject_code(sent)\n",
    "    doc = expand_nlp(sent)\n",
    "    doc = constituency_nlp(doc)\n",
    "    doc = structure_nlp(doc)\n",
    "    j = extract_doc(doc)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2860,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all courses\n",
    "# courses = list(\n",
    "#     catalog.get_collection(\"courses\").find(\n",
    "#         {\"prereq\": {\"$ne\": None}, \"career\": \"Undergraduate Programs\", \"active\": True}\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# courses_prereq = catalog.get_collection(\"courses_prereq\")\n",
    "# courses_prereq.delete_many({})\n",
    "\n",
    "# for course in courses:\n",
    "#     prereq = course[\"prereq\"]\n",
    "\n",
    "#     if prereq:\n",
    "#         print(prereq)\n",
    "\n",
    "#         result = try_nlp(course, prereq)\n",
    "\n",
    "#         print(result)\n",
    "#         print(\"\")\n",
    "\n",
    "#         courses_prereq.insert_one(\n",
    "#             {\"course\": course[\"code\"], \"prereq_text\": prereq, \"prereq\": result}\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
